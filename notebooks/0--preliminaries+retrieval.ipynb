{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7fe3038",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "Here, we explain briefly how we organise our notebooks. Throughout all notebooks, we shall employ a custom module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94275c",
   "metadata": {},
   "source": [
    "It contains a configuration dictionary named `CFG` which holds global environment variables. This shall facilitate the process of storing and loading data between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc373c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = ipynb_utils.CFG\n",
    "\n",
    "padding_length = max([len(k) for k in CFG])\n",
    "\n",
    "print(\"CFG Dictionary:\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"  {k:{padding_length}} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079ddc3",
   "metadata": {},
   "source": [
    "Finally, let us delineate the contents of our notebooks and the data upon which they respectively depend:\n",
    "\n",
    "- `0--preliminaries+retrieval.ipynb`: This is the present notebook. The code that retrieves the data may be found in the subsequent section. The resulting file `df_processed.pkl` is requisite for all subsequent notebooks.\n",
    "- `1--analysis.ipynb`: Performs basic exploratory data analysis and produces two processed data frames:\n",
    "  - In the middle of the analysis, the file `df_duplicate.pkl` is produced, which is required for the excursory side analysis in `archive/1--analysis_hint-ignorant.ipynb`.\n",
    "  - The file `df_processed.pkl`, produced at the end, is requisite for the notebook on modelling.\n",
    "- `2--model.ipynb`: Contains the predictive Machine Learning models.\n",
    "\n",
    "In the `archive/` directory, the reader may find two additional notebooks:\n",
    "\n",
    "- `0--retrieval_kaggle.ipynb`: Describes the procedure to download the data via kaggleâ€”with the required credentials.\n",
    "- `1--analysis_hint-ignorant.ipynb`: Presents a data analysis in which we deliberately disregard the fact that the instructors intended to include a data duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a15ee",
   "metadata": {},
   "source": [
    "# Data Retrieval\n",
    "\n",
    "In this notebook, we shall provide the code required to retrieve the data intended for analysis. It should be noted that appropriate credentials must be supplied to access the database.\n",
    "\n",
    "We shall employ the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191764c8",
   "metadata": {},
   "source": [
    "Subsequently, several useful variables for the retrieval process are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f64a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CFG[\"DATA_DIR\"]\n",
    "\n",
    "# Paths to which dataframes will be saved.\n",
    "DF_PKL_PATH_TAR = os.path.join(DATA_DIR, \"df_raw.pkl\")\n",
    "DF_CSV_PATH_TAR = os.path.join(DATA_DIR, \"df_raw.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7faf220",
   "metadata": {},
   "source": [
    "A brief examination of the given database, conducted within a GUI application such as DBeaver, reveals that the following SQL query yields the correct dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "    SET SCHEMA 'diabetes';\n",
    "\n",
    "    SELECT * FROM\n",
    "            patient p\n",
    "        LEFT JOIN\n",
    "            blood_metrics bm\n",
    "        ON p.id = bm.patientid\n",
    "        LEFT JOIN\n",
    "            pedigree_outcome po\n",
    "        ON p.id = po.patientid\n",
    "        LEFT JOIN\n",
    "            skin s\n",
    "        ON p.id = s.patientid\n",
    "    ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb44397",
   "metadata": {},
   "source": [
    "We load the credentials required for database access from the `.env` file into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"name\": os.getenv(\"DB_NAME\"),\n",
    "    \"scheme\": os.getenv(\"DB_SCHEME\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\")\n",
    "}\n",
    "\n",
    "DB_URI = (\n",
    "    \"{scheme}://{user}:{password}@{host}:{port}/{name}\"\n",
    "    .format(**DB_CONFIG)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b10ec9",
   "metadata": {},
   "source": [
    "We establish a connection with the database and load the data specified by the query into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = create_engine(DB_URI)\n",
    "\n",
    "with db.connect() as conn:\n",
    "    df = pd.read_sql(QUERY, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f678d10",
   "metadata": {},
   "source": [
    "Let us confirm that the download process has been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc60988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240154c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493bd8b",
   "metadata": {},
   "source": [
    "The join operation in the SQL query may cause duplication of columns. Furthermore, the column `\"patientid\"` is identical to `\"id\"`. Such duplicates ought to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "if \"patientid\" in df.columns:\n",
    "    df = df.drop(columns=[\"patientid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba19e73",
   "metadata": {},
   "source": [
    "At last, the data frame is ready to be stored: To preserve the original data structure, we employ a pickle file; the csv version serves solely for direct visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aef3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(DF_PKL_PATH_TAR)\n",
    "df.to_csv(DF_CSV_PATH_TAR, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
