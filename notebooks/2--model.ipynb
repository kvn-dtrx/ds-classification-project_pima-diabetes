{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89ed039",
   "metadata": {},
   "source": [
    "# Prediction Model\n",
    "\n",
    "In this notebook, we construct predictive models based on the dataset prepared in the [preceding notebook](./1--analysis.ipynb).\n",
    "\n",
    "To evaluate the quality of the models, we select the true positive rate (i.e. recall for the presence of diabetes) as our metric, as an initial educated guess: it shall be deemed more serious to misclassify individuals afflicted with the disease as healthy than to ascribe diabetes to healthy individuals.\n",
    "\n",
    "Naturally, this choice might require refinement should more information (e.g. regarding the severity of side effects from prescribed drugs) be available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd6db7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this notebook, the following modules are requisite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipynb_utils import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122d719",
   "metadata": {},
   "source": [
    "Let us encapsulate certain procedures to be applied in the error analysis of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_palette = {\n",
    "    \"TP\": \"green\",\n",
    "    \"TN\": \"blue\",\n",
    "    \"FP\": \"red\",\n",
    "    \"FN\": \"orange\",\n",
    "}\n",
    "\n",
    "\n",
    "def classify_outcome(row):\n",
    "    if row[\"diagnosis\"] == 1:\n",
    "        if row[\"prognosis\"] == 1:\n",
    "            return \"TP\" \n",
    "        else:\n",
    "            return \"FN\"\n",
    "    else:\n",
    "        if row[\"prognosis\"] == 1:\n",
    "            return \"FP\"\n",
    "        else:\n",
    "            return \"TN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf3d61",
   "metadata": {},
   "source": [
    "The following code cell gathers the environment variables to be used explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de586a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSEED = CFG[\"RSEED\"]\n",
    "\n",
    "DATA_DIR = CFG[\"DATA_DIR\"]\n",
    "\n",
    "DF_PKL_PATH_SRC = os.path.join(DATA_DIR, \"df_processed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13452c2",
   "metadata": {},
   "source": [
    "Let us reload the processed data from preceding exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6860486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(DF_PKL_PATH_SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b931a",
   "metadata": {},
   "source": [
    "Prior to the imputation of implausible values in the data frame, a pseudo-feature named `\"is_test\"` was already introduced to implement a train-test split. Naturally, utilising any data division risks other than this risks information leakage; accordingly, we shall adhere to this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d289e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_test = df[\"is_test\"] != 0\n",
    "\n",
    "# Train data\n",
    "df_0 = df[~is_test]\n",
    "# Test data\n",
    "df_1 = df[is_test]\n",
    "\n",
    "X_cols_blacklist = [\n",
    "    \"id\",\n",
    "    \"has_diabetes\",\n",
    "    \"is_test\",\n",
    "]\n",
    "\n",
    "# Genuine features\n",
    "X_cols = [col for col in df.columns if col not in X_cols_blacklist]\n",
    "# Target\n",
    "y_col = \"has_diabetes\"\n",
    "\n",
    "# Train data, feature part\n",
    "X_0 = df_0[X_cols]\n",
    "# Test data, feature part\n",
    "X_1 = df_1[X_cols]\n",
    "# Train data, target part\n",
    "y_0 = df_0[y_col]\n",
    "# Test data, target part\n",
    "y_1 = df_1[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc4e91",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression Model\n",
    "\n",
    "For the purpose of benchmarking, we select a logistic regression model that prognosticates the presence or absence of diabetes solely upon `\"age\"` and `\"bmi\"`.\n",
    "\n",
    "To attain more accurate results, we shall apply a pipeline including oversampling and scaling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2229e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols_red = [\"age\", \"bmi\"]\n",
    "\n",
    "# ColumnTransformer to select columns.\n",
    "feature_selector = ColumnTransformer([(\"selector\", \"passthrough\", X_cols_red)])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        # Feature selection\n",
    "        (\"feature_selector\", feature_selector),\n",
    "        # Oversampler\n",
    "        (\"oversampler\", RandomOverSampler(random_state=RSEED)),\n",
    "        # Scaler for features (target scaling is not necessary)\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        # Logistic regression\n",
    "        (\"model\", LogisticRegression(random_state=RSEED)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb0228",
   "metadata": {},
   "source": [
    "The following code cell fits the pipeline to the processed data and computes the parameters of the unscaled features within the linear regression model it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc215496",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid={},\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_0, y_0)\n",
    "\n",
    "estimator = grid_search.best_estimator_\n",
    "\n",
    "steps = estimator.named_steps\n",
    "\n",
    "model = steps[\"model\"]\n",
    "# Coefficients and intercept are computed with respect\n",
    "# to the scaled features.\n",
    "coef_scaled = model.coef_[0]\n",
    "intercept_scaled = model.intercept_[0]\n",
    "\n",
    "scaler = steps[\"scaler\"]\n",
    "data_min = scaler.data_min_ \n",
    "data_max = scaler.data_max_ \n",
    "data_range = data_max - data_min \n",
    "\n",
    "# Back-transformation of coefficients and intercept to original scale\n",
    "coef_orig = coef_scaled / data_range\n",
    "intercept_orig = intercept_scaled - np.sum(coef_scaled * data_min / data_range)\n",
    "\n",
    "print(\"Best parameters :\")\n",
    "print(f\"  Coefficients : {coef_orig}\")\n",
    "print(f\"  Intercept    : {intercept_orig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3996741",
   "metadata": {},
   "source": [
    "The coefficients for the features are positive, as one would naively expect: the older an individual, the more likely he is to suffer from diabetes; an analogous argument applies to BMI.\n",
    "\n",
    "We proceed with the evaluation of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = estimator.predict(X_1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_1, z_1))\n",
    "\n",
    "tpr = recall_score(y_1, z_1, pos_label=1)\n",
    "print(f\"True Positive Rate (Recall): {tpr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c3d3e",
   "metadata": {},
   "source": [
    "The resulting confusion matrix has the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_1, z_1)\n",
    "sns.heatmap(cm, cmap=\"YlGnBu_r\", annot=True, fmt=\".0f\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fb518",
   "metadata": {},
   "source": [
    "To visualise the error, we plot all test data points coloured by their actual and predicted outcomes in the `\"age\"`–`\"bmi\"` plane. Moreover, we may include the decision boundary of the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ac87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot part\n",
    "\n",
    "# Frame with plotting data\n",
    "df_plot = X_1[[\"age\", \"bmi\"]].copy()\n",
    "df_plot[\"diagnosis\"] = y_1\n",
    "df_plot[\"prognosis\"] = z_1\n",
    "df_plot[\"outcome\"] = df_plot.apply(classify_outcome, axis=1)\n",
    "\n",
    "# Scatterplot\n",
    "sns.scatterplot(\n",
    "    data=df_plot,\n",
    "    x=\"age\",\n",
    "    y=\"bmi\",\n",
    "    hue=\"outcome\",\n",
    "    palette=outcome_palette,\n",
    "    s=20,\n",
    "    edgecolor=\"k\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Decision boundary part\n",
    "\n",
    "# The decision boundary is described by \n",
    "# age_coeff * age + bmi_coeff * bmi + intercept = 0.\n",
    "# We resolve this equation for bmi.\n",
    "\n",
    "# Range of age\n",
    "xx = np.array([df_plot[\"age\"].min() - 1, df_plot[\"age\"].max() + 1])\n",
    "\n",
    "# Corresponding bmi values on the decision boundary line\n",
    "yy = -(intercept_orig + coef_orig[0] * xx) / coef_orig[1]\n",
    "\n",
    "# Plot decision boundary line\n",
    "plt.plot(xx, yy, \"k--\", linewidth=2, label=\"Decision Boundary\")\n",
    "\n",
    "# Finalisation of the plot\n",
    "\n",
    "plt.xlabel(\"Age (years)\")\n",
    "plt.ylabel(\"BMI (kg/sqm)\")\n",
    "plt.title(\"Classification Results with Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ed7de",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "\n",
    "As the Machine Learning model intended to optimise our target evaluation metric, we choose a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols_blacklist = [\"dpf\", \"date\"]\n",
    "X_cols_red = [col for col in X_cols if col not in X_cols_blacklist]\n",
    "\n",
    "feature_selector = ColumnTransformer([(\"selector\", \"passthrough\", X_cols_red)])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        # Feature selection\n",
    "        (\"feature_selector\", feature_selector),\n",
    "        # Oversampler\n",
    "        (\"oversampler\", RandomOverSampler(random_state=RSEED)),\n",
    "        # Scaling is not strictly necessary for Decision Trees\n",
    "        # (\"scaler\", MinMaxScaler()),\n",
    "        # Decision tree classifier\n",
    "        (\"model\", DecisionTreeClassifier(random_state=RSEED)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662bd8dc",
   "metadata": {},
   "source": [
    "We shall fit the model to the training data. A parameter grid serves to identify a superior variant of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bf445",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__max_depth\": [2, 4, 8, 16, None],\n",
    "    \"model__min_samples_split\": [2, 4, 8],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4, 8],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_0, y_0)\n",
    "\n",
    "print(\"Best parameters :\")\n",
    "pad = max([len(k) for k in param_grid]) + 1\n",
    "for k,v in grid_search.best_params_.items():\n",
    "    print(f\"  {k:{pad}}: {v}\")\n",
    "print(\"Best CV score   :\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe73f7",
   "metadata": {},
   "source": [
    "We shall print the evaluation report of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = grid_search.predict(X_1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_1, z_1))\n",
    "\n",
    "tpr = recall_score(y_1, z_1, pos_label=1)\n",
    "print(f\"True Positive Rate (Recall): {tpr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283449d",
   "metadata": {},
   "source": [
    "Once more, let us display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8424134",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_1, z_1)\n",
    "sns.heatmap(cm, cmap=\"YlGnBu_r\", annot=True, fmt=\".0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee86818",
   "metadata": {},
   "source": [
    "To complete the parallelism with the error evaluation of the baseline model, we plot all test data points coloured by their actual and predicted outcomes in the `\"age\"`–`\"bmi\"` plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame with plotting data.\n",
    "df_plot = X_1[[\"age\", \"bmi\"]].copy()\n",
    "df_plot[\"diagnosis\"] = y_1\n",
    "df_plot[\"prognosis\"] = z_1\n",
    "df_plot[\"outcome\"] = df_plot.apply(classify_outcome, axis=1)\n",
    "\n",
    "# Scatterplot\n",
    "sns.scatterplot(\n",
    "    data=df_plot,\n",
    "    x=\"age\",\n",
    "    y=\"bmi\",\n",
    "    hue=\"outcome\",\n",
    "    palette=outcome_palette,\n",
    "    s=20,\n",
    "    edgecolor=\"k\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Finalisation of the plot\n",
    "\n",
    "plt.xlabel(\"Age (years)\")\n",
    "plt.ylabel(\"BMI (kg/sqm)\")\n",
    "plt.title(\"Classification Results with Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ec6d3",
   "metadata": {},
   "source": [
    "One may clearly observe that, in contrast to the baseline model, there is no longer a \"decision boundary\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
