{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines or how to preprocess your data in a clean and organized fashion\n",
    "\n",
    "![](https://media1.faz.net/ppmedia/aktuell/83311481/1.1703919/default-retina/der-untergang-der-titanic-1912.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pipelines](https://scikit-learn.org/stable/modules/compose.html#pipeline) are a useful tool for going through a whole sequence of data processing and modeling steps in the right order and offer three main advantages:\n",
    "\n",
    "- **Convenience and encapsulation** \n",
    "\n",
    "    You only have to call `.fit()`and `.predict()`once to fit a whole sequence of processing steps.\n",
    "- **Grid Search Hyperparemeter Selection over all Hyperparamters in pipeline possible at once** \n",
    "- **Safety** \n",
    "\n",
    "    Pipelines help avoid leaking statistics from your test data into model training. \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "We will show how to create and use a pipeline on the titanic dataset. Therefore we will start by loading the relevant packages and the dataset. Since you've already worked with this dataset, we will skip the data exploration part. This notebook will focus on how to build a pipeline for effectively preprocessing this dataset and tuning the hyperparameters using grid search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:21:05.166035Z",
     "start_time": "2020-06-25T08:21:02.246018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import of relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set random seed \n",
    "RSEED = 42\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:24:15.580259Z",
     "start_time": "2020-06-25T08:24:15.446522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the titanic dataset\n",
    "df = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load now the fool dataset with all the columns\n",
    "\n",
    "**Variable Description:**\n",
    "\n",
    "|Variable|Definition   | Key  |  Type |\n",
    "|---|---|---|---|\n",
    "| Survived | Survival   |   0 = No, 1 = Yes | dichotomous | \n",
    "|Pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|ordinal|\n",
    "|Sex|Sex||dichotomous|\n",
    "|Age|Age|in years|ratio|\n",
    "|SibSp|# of siblings / spouses aboard the Titanic|\t|ratio|\n",
    "|Parch|# of parents / children aboard the Titanic|  |ratio|\n",
    "|Ticket|Ticket number||nominal|\n",
    "|Fare|Passenger fare||ratio|\n",
    "|Cabin|Cabin number||nominal|\n",
    "|Embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|nominal|  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Before we begin to build our pipeline let's have a quick look at the data to refresh our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:24:36.030825Z",
     "start_time": "2020-06-25T08:24:36.011470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting an idea of the dimension\n",
    "print('Number of rows and columns of train: ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:24:40.414321Z",
     "start_time": "2020-06-25T08:24:40.343567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the tail of the dataset\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:24:46.328878Z",
     "start_time": "2020-06-25T08:24:46.281657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the type of features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:26:07.557839Z",
     "start_time": "2020-06-25T08:26:07.439677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Having a look at some simple, descriptive statistics \n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:25:02.040768Z",
     "start_time": "2020-06-25T08:25:02.000822Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many unique entries do the features have?\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:25:20.073054Z",
     "start_time": "2020-06-25T08:25:20.014098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "missing = pd.DataFrame(df.isnull().sum(), columns=[\"Amount\"])\n",
    "missing['Percentage'] = round((missing['Amount']/df.shape[0])*100, 2)\n",
    "missing[missing['Amount'] != 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 features with missing values.\n",
    "\n",
    "* **Age**  \n",
    "* **Cabin**\n",
    "* **Embarked**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T08:42:32.024009Z",
     "start_time": "2020-06-22T08:42:32.018472Z"
    }
   },
   "source": [
    "---\n",
    "## Building a Preprocessing Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the modeling part we will concentrate on a few promising features. We will drop the features **PassengerId**, **Name**, **Cabin** and **Ticket**. \n",
    " * The **PassengerId** does not contain helpful information and \n",
    " * for the feature **Cabin** there are over 77% values missing. \n",
    " * **Name** and **Ticket** might contain helpful information but we need to extract them via feature engineering. \n",
    " \n",
    " Feel free to play around with those: maybe you can create new features which will further improve your models. But for now we'll stick to the remaining ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:28:03.378439Z",
     "start_time": "2020-06-25T08:28:03.366493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping the unnecessary columns \n",
    "df.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical vs numerical variables\n",
    "\n",
    "Before we start building our pipeline we create a list, which contains the features we want to use for the modeling process. Since categorical and numerical features need to be preprocessed differently, we split the features in two lists: one for categorical and one for numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:28:04.191555Z",
     "start_time": "2020-06-25T08:28:04.169972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating list for categorical predictors/features \n",
    "# (dates are also objects so if you have them in your data you would deal with them first)\n",
    "cat_features = list(df.columns[df.dtypes==object])\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:28:37.916898Z",
     "start_time": "2020-06-25T08:28:37.871277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating list for numerical predictors/features\n",
    "# Since 'Survived' is our target variable we will exclude this feature from this list of numerical predictors \n",
    "num_features = list(df.columns[df.dtypes!=object])\n",
    "num_features.remove('Survived')\n",
    "num_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split\n",
    "\n",
    "Let's split the data set into a training and test set. Using the training set and cross validation we will train our model and find the best hyperparameter combination. In the end the test set will be used for the final evaluation of our best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:29:02.037573Z",
     "start_time": "2020-06-25T08:29:02.011057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define predictors and target variable\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "print(f\"We have {X.shape[0]} observations in our dataset and {X.shape[1]} features\")\n",
    "print(f\"Our target vector has also {y.shape[0]} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:29:05.099306Z",
     "start_time": "2020-06-25T08:29:05.082576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:29:06.535780Z",
     "start_time": "2020-06-25T08:29:06.521996Z"
    }
   },
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sk_pipeline.png)\n",
    "\n",
    "Building a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) always follows the same syntax. In our case we create one pipeline for our numerical features and one for our categorical features. \n",
    "\n",
    "The missing values of the numerical features should be filled with the median value of the features and in the end, each feature should be scaled using the StandardScaler.\n",
    "\n",
    "The missing values of the categorical features should be changed to \"missing\". In the end, we encode all categorical features as a one-hot numeric array. \n",
    "\n",
    "\n",
    "In the end both pipelines are combined into one pipeline called \"preprocessor\" using [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:34:33.902990Z",
     "start_time": "2020-06-25T08:34:33.869882Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipline for numerical features\n",
    "# Initiating Pipeline and calling one step after another\n",
    "# each step is built as a list of (key, value)\n",
    "# key is the name of the processing step\n",
    "# value is an estimator object (processing step)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:34:50.607057Z",
     "start_time": "2020-06-25T08:34:50.581016Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Complete pipeline for numerical and categorical features\n",
    "# 'ColumnTransformer' applies transformers (num_pipeline/ cat_pipeline)\n",
    "# to specific columns of an array or DataFrame (num_features/cat_features)\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modelling using Pipelines and Grid Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Now that we have a preprocessing pipeline we can add a model on top (this sequence will also be handled by a Pipeline) and see how it performs using cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:36:41.885876Z",
     "start_time": "2020-06-25T08:36:41.868702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building a full pipeline with our preprocessor and a LogisticRegression Classifier\n",
    "pipe_logreg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:38:04.935253Z",
     "start_time": "2020-06-25T08:38:04.528252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "# cross_val_predict expects an estimator (model), X, y and nr of cv-splits (cv)\n",
    "y_train_predicted = cross_val_predict(pipe_logreg, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:38:13.734248Z",
     "start_time": "2020-06-25T08:38:13.708004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the accuracy for the LogisticRegression Classifier \n",
    "print('Cross validation scores:')\n",
    "print('-------------------------')\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_train_predicted)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_train_predicted)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_train_predicted)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing via Grid Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize our model we will use gird search. At first we have to define a parameter space we want to search for the best parameter combination. Then we have to initiate our grid search via GridSearchCV. The last step is to use the fit method providing our training data as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:39:46.702896Z",
     "start_time": "2020-06-25T08:39:46.691148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining parameter space for grid-search. Since we want to access the classifier step (called 'logreg') in our pipeline \n",
    "# we have to add 'logreg__' in front of the corresponding hyperparameters. \n",
    "param_logreg = {'logreg__penalty':('l1','l2'),\n",
    "                'logreg__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "                'logreg__solver': ['liblinear', 'lbfgs', 'sag']\n",
    "               }\n",
    "\n",
    "grid_logreg = GridSearchCV(pipe_logreg, param_grid=param_logreg, cv=5, scoring='accuracy', \n",
    "                           verbose=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:41:19.987624Z",
     "start_time": "2020-06-25T08:41:14.183118Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:43:09.780665Z",
     "start_time": "2020-06-25T08:43:09.755280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_logreg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_logreg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model (including fitted preprocessing steps) as best_model \n",
    "best_model = grid_logreg.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation\n",
    "\n",
    "Finally we have a good model. Let's see if it also passes the final evaluation on the test data. Therefore we have to prepare the test set in the same way we did with the training data. Thanks to our pipeline it's done in a blink and we can be sure no data-leakage happened at any step through the whole data preprocessing.\n",
    "\n",
    "When we saved the best model in the cell above, we did not only save the trained model but also the fitted preprocessing pipeline. Thus, transforming the test data the same way as the train data happens also when calling the `.predict` method on the `best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:45:08.221609Z",
     "start_time": "2020-06-25T08:45:08.175840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the accuracy, recall and precision for the test set with the optimized model\n",
    "y_test_predicted = best_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_test_predicted)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might want to transform your features in a very specific way, which is not implemented in scikit-learn yet. In those cases you can create your very own custom transformers. In order to work seamlessly with everything scikit-learn provides you need to create a class and implement the three methods `.fit()`, `.transform()` and `.fit_transform()`.      \n",
    "Two useful base classes on which you can construct your personal transformer can be imported with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T09:25:40.872499Z",
     "start_time": "2020-06-24T09:25:40.849696Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about building your own transformers or pipelines in general I would recommend to have a look at the following books:\n",
    "\n",
    "**Introduction to Machine Learning with Python by Müller and Guido (2017), Chapter 6       \n",
    "Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow by Geron (2019), Chapter 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
